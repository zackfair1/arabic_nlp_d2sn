{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Classification Training</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from matplotlib import pyplot as plt\n",
    "import os, re, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read csv inside zip file for pandas\n",
    "def extract_df(url):\n",
    "    # Necessary packages\n",
    "    import re, requests, shutil, zipfile, os\n",
    "    import pandas as pd\n",
    "    # The url where we can fide the file\n",
    "    url_df = url\n",
    "    # The name of the zip file\n",
    "    file_df = re.findall(r'([^\\/]+$)', url)[0]\n",
    "    if '.zip' not in file_df:\n",
    "        files_dir = file_df+'.zip'\n",
    "    else:\n",
    "        files_dir = file_df\n",
    "    if not os.path.exists(files_dir):\n",
    "        os.mkdir(files_dir)\n",
    "    # Command to donwload the file at the given url\n",
    "    r = requests.get(url_df)\n",
    "    # Then we open the file\n",
    "    open(file_df, 'wb').write(r.content)\n",
    "    # We extract the content of the .zip file\n",
    "    with zipfile.ZipFile(file_df, 'r') as unzip:\n",
    "        unzip.extractall(files_dir)\n",
    "     # we finally read the csv and make some cleaning\n",
    "    df = pd.read_csv(os.path.join(\n",
    "        files_dir, [i for i in unzip.namelist() if i.endswith('.csv')][0]))\n",
    "    # We remove files/dir\n",
    "    shutil.rmtree(files_dir)\n",
    "    os.remove(file_df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataframe\n",
    "df = extract_df('https://data.mendeley.com/public-files/datasets/v524p5dhpj/files/72c2e306-9538-4c74-a28f-558fbe87c382/file_downloaded').rename(columns={'targe':'target'})\n",
    "\n",
    "# Train test split\n",
    "texts = df['text']\n",
    "target = df['target']\n",
    "text_train, text_test, target_train, target_test = train_test_split(texts, target, test_size=.2, random_state=23)\n",
    "\n",
    "# # Or with datasets\n",
    "# dataset = Dataset.from_pandas(df).train_test_split(test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Build a tokenizer</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Main files for the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create dir for the files (for tokenizer creation)\n",
    "try:\n",
    "    shutil.rmtree('files')\n",
    "    os.mkdir('files')\n",
    "except:\n",
    "    print('File already exists')\n",
    "\n",
    "for i, text in enumerate(tqdm(texts)):\n",
    "    with open(f'files/text_{i}.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the files for the tokenizer\n",
    "get_files = lambda directory : [file for dir,_,files in os.walk(directory) for file in files]\n",
    "files = get_files(r'files/')\n",
    "# For sorting\n",
    "files.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "# Append the directory to the files\n",
    "files = [f'files/{i}' for i in files]\n",
    "\n",
    "# # # Once done\n",
    "# # shutil.rmtree('files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Main tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3arabert\\\\vocab.json', '3arabert\\\\merges.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ~ 8 minutes of training\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=files, special_tokens=['<s>','<pad>','</s>','<unk>','<mask>'])\n",
    "\n",
    "# Save the model\n",
    "try:\n",
    "    os.mkdir('3arabert')\n",
    "except:\n",
    "    print('Folder already exists')\n",
    "tokenizer.save_model('3arabert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file 3arabert\\config.json not found\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n",
      "file 3arabert\\config.json not found\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0, 265, 1426, 18526, 2],\n",
       " '<s>السلام عليكم</s>',\n",
       " [0, 29768, 299, 1184, 2],\n",
       " '<s>سبحان الله</s>')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the generated tokenizer\n",
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('3arabert')\n",
    "\n",
    "# Test\n",
    "tokenizer.encode('السلام عليكم'), tokenizer.decode([0, 265, 1426, 18526, 2]), tokenizer.encode('سبحان الله'), tokenizer.decode([0, 29768, 299, 1184, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Masked language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Masked language model function for torch\n",
    "def mlm(tensor):\n",
    "    rand = torch.rand(tensor.shape)\n",
    "    mask_arr = (rand < 0.15) * (tensor > 2)\n",
    "    for i in range(tensor.shape[0]):\n",
    "        selection = torch.flatten(mask_arr[i].nonzero())\n",
    "        tensor[i, selection] = 4\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105586/105586 [03:55<00:00, 448.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# ~ 4 minutes\n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "labels = []\n",
    "\n",
    "for file in tqdm(files):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    sample = tokenizer(lines, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    labels.append(sample.input_ids)\n",
    "    attention_mask.append(sample.attention_mask)\n",
    "    input_ids.append(mlm(sample.input_ids.detach().clone()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Concatenate the tensors into one tensor for all (input ids, attention masks and lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>بين أستوديوهات ورزازات وصحراء مرزوكة وآثار وليلي ثم الرباط والبيضاء انتهى المخرج المغربي سهيل بن بركة من تصوير مشاهد عمله السينمائي الجديد الذي خصصه لتسليط الضوء عن حياة الجاسوس الإسباني دومينغو باديا الذي عاش فترة من القرن التاسع عشر بالمغرب باسم علي باي هذا الفيلم الذي اختار له مخرجه عنوان حلم خليفة يصور حياة علي باي العباسي الذي ما زال أحد أحياء طنجة يحمل اسمه عاش حياة فريدة متنكرا بشخصية تاجر عربي من سلالة الرسول صلى الله عليه وسلم فيما كان يعمل جاسوسا لحساب إسبانيا وكشف مخرج الفيلم سهيل بن بركة في تصريح لهسبريس أن الفيلم السينمائي دخل مرحلة التوضيب التي تتم خارج المغرب مبرزا أن الفيلم الذي يروي حياة الجاسوس الإسباني دومينغو باديا منذ أن قرر من طنجة بدء رحلاته نحو عدد من المناطق في العالم الإسلامي بداية القرن العشرين سيكون جاهزا بعد شهرين ويجمع الفيلم السينمائي عددا من الممثلين من مختلف الجنسيات واختار لدور البطولة الممثلة السينمائية الإيطالية كارولينا كريشنتيني للقيام بدور الإنجليزية الليدي هستر ستانهوب التي اشتهرت في الكتب الغربية بـ زنوبيا والتي عاشت بدورها بالدول العربية وارتبطت بعلي باي بعلاقة عاطفية إضافة إلى وجوه سينمائية معروفة وعن اختيار المخرج المغربي لحياة علي باي العباسي يوضح في تصريح لوكالة الأنباء الفرنسية هذه الشخصية عاشت أحداثا مشوقة كثيرة تستحق أن تسلط عليها الأضواء مشيرا إلى أن الفيلم سيحمل الكثير من المفاجآت لا سيما أن البطل قتل على يد امرأة دست له السم خلال رحلة الحج وأضاف شخصية طموحة وشجاعة ومثقفة ومذهلة في آن واحد كان يرى نفسه مستكشفا في أول الأمر نال علي باي إعجاب السلطان بعلمه فجعله من المقربين منه في ظرف وجيز ودعاه إلى اللحاق به إلى فاس وبرحيله إلى فاس تنتهي قصته مع طنجة وعاش علي باي العباسي بمدينة طنجة على أنه رجل مسلم أصله من الشام ونال ثقة الجميع في هذه المدينة حيث تم تشييد تمثال له في عروسة الشمال نظرا لتمكنه من بعض العلوم خاصة علم الفلك الذي مكنه من رصد كسوف الشمس الذي تزامن مع وجوده في طنجة فكان لعلمه دور كبير ساعده في إخفاء هويته كما أبان هذا الأمر أيضا عن تراجع كبير في ميدان العلم والمعرفة لدى المغاربة والمسلمين بصفة عامة</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_input_ids = torch.cat(input_ids)\n",
    "final_attention_mask = torch.cat(attention_mask)\n",
    "final_labels = torch.cat(labels)\n",
    "\n",
    "tokenizer.decode(final_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>قررت النجمة الأمريكية أوبرا وينفري ألا يقتصر عملها على الفن بل عملت مع أحد المتخصصين لإطلاق نوع جديد من الشاي سيصبح متوفرا ابتداء من الشهر المقبل في سلسلة مقاهي ستاربكس ونقلت وسائل إعلام أمريكية عن رئيس مجلس إدارة ستاربكس هاورد شولتز ووينفري إعلانهما عن ابتكار نوع جديد من الشاي يحمل اسم الذي سيباع ابتداء من أبريل نيسان المقبل في مقاهي ستاربكس وتيفانا بأمريكا وكندا وتعتزم ستاربكس التبرع بعائدات بيع هذا الشاي لأكاديمية أسستها وينفري وتعنى بتوفير فرص تعليم للشبان</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Main encodings\n",
    "encodings = {\n",
    "    'input_ids': final_input_ids,\n",
    "    'attention_mask': final_attention_mask,\n",
    "    'labels': final_labels\n",
    "}\n",
    "\n",
    "# Dataset class\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}\n",
    "\n",
    "# Main dataset\n",
    "dataset=Dataset(encodings=encodings)\n",
    "dataset\n",
    "# tokenizer.decode(dataset[3]['input_ids'])\n",
    "\n",
    "# Dataloader for batch and sampling...\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True) # reducing batch size for the gpu memory allocation error\n",
    "tokenizer.decode(dataloader.dataset[1]['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training the model</h3>\n",
    "<h5>Setup the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "config = RobertaConfig(\n",
    "    vocab_size = tokenizer.vocab_size,\n",
    "    max_position_embeddings = 514,\n",
    "    hidden_size = 768,\n",
    "    num_attention_heads = 12,\n",
    "    num_hidden_layers = 6, # Deep learning layers\n",
    "    type_vocab_size=1 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Initialize the model<h5>\n",
    "In case of an error when using cuda, just switch to CPU then again to Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary())\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the GPU\n",
    "torch.cuda.empty_cache()\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Main model\n",
    "from transformers import RobertaForMaskedLM\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zacke\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Typical Adam optimiser with a learning rate of 1e-5\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26397/26397 [1:37:52<00:00,  4.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Epochs --- ~ 1 / 2 hours\n",
    "epochs = 2\n",
    "print(dataloader.batch_size)\n",
    "# Loop instantiation for dataloader\n",
    "loop = tqdm(dataloader, leave=True)\n",
    "for batch in loop:\n",
    "    # Reset the gradients explicitely after each loop\n",
    "    optimizer.zero_grad()\n",
    "    final_input_ids = batch['input_ids'].to(device)\n",
    "    final_attention_mask = batch['attention_mask'].to(device)\n",
    "    final_labels = batch['labels'].to(device)\n",
    "    output = model(input_ids=final_input_ids, \n",
    "                   attention_mask=final_attention_mask,\n",
    "                   labels=final_labels)\n",
    "    loss = output.loss\n",
    "    # For backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # To view the progress\n",
    "    # loop.set_description(f'Epoch: {epoch}')\n",
    "    # loop.set_postfix(loss=loss.items())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model after training\n",
    "# model.save_pretrained('3arabert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "tkzr = RobertaTokenizer.from_pretrained('3arabert',max_len=512)\n",
    "mdl = RobertaForMaskedLM.from_pretrained('3arabert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at 3arabert were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at 3arabert and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "test = pipeline('text-classification', model='3arabert', tokenizer='3arabert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.53694087266922}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(f'هل قمت بذلك أم أنت لست على هواك؟ كيف يمكنك فعل ذلك؟؟')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer & Model\n",
    "from transformers import GPT2TokenizerFast, pipeline\n",
    "\n",
    "MODEL_NAME='aubmindlab/aragpt2-base'\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(num_words=None,lower=False)\n",
    "# tokenizer.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 267. GiB for an array with shape (84468, 423535) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zacke\\OneDrive - vgytk\\D2SN\\NLP & Analyse textuelle\\arabic\\3.text_classification_sentiment\\training\\training.ipynb Cell 8'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zacke/OneDrive%20-%20vgytk/D2SN/NLP%20%26%20Analyse%20textuelle/arabic/3.text_classification_sentiment/training/training.ipynb#ch0000009?line=0'>1</a>\u001b[0m x_train \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mtexts_to_matrix(text_train, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtfidf\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zacke/OneDrive%20-%20vgytk/D2SN/NLP%20%26%20Analyse%20textuelle/arabic/3.text_classification_sentiment/training/training.ipynb#ch0000009?line=1'>2</a>\u001b[0m x_test \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mtexts_to_matrix(target_train, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtfidf\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras_preprocessing\\text.py:383\u001b[0m, in \u001b[0;36mTokenizer.texts_to_matrix\u001b[1;34m(self, texts, mode)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/keras_preprocessing/text.py?line=372'>373</a>\u001b[0m \u001b[39m\"\"\"Convert a list of texts to a Numpy matrix.\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/keras_preprocessing/text.py?line=373'>374</a>\u001b[0m \n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/keras_preprocessing/text.py?line=374'>375</a>\u001b[0m \u001b[39m# Arguments\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/keras_preprocessing/text.py?line=379'>380</a>\u001b[0m \u001b[39m    A Numpy matrix.\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/keras_preprocessing/text.py?line=380'>381</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/keras_preprocessing/text.py?line=381'>382</a>\u001b[0m sequences \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtexts_to_sequences(texts)\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/keras_preprocessing/text.py?line=382'>383</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msequences_to_matrix(sequences, mode\u001b[39m=\u001b[39;49mmode)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras_preprocessing\\text.py:413\u001b[0m, in \u001b[0;36mTokenizer.sequences_to_matrix\u001b[1;34m(self, sequences, mode)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/keras_preprocessing/text.py?line=408'>409</a>\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtfidf\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdocument_count:\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/keras_preprocessing/text.py?line=409'>410</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mFit the Tokenizer on some data \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/keras_preprocessing/text.py?line=410'>411</a>\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mbefore using tfidf mode.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/keras_preprocessing/text.py?line=412'>413</a>\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mzeros((\u001b[39mlen\u001b[39;49m(sequences), num_words))\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/keras_preprocessing/text.py?line=413'>414</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, seq \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sequences):\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python39/lib/site-packages/keras_preprocessing/text.py?line=414'>415</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m seq:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 267. GiB for an array with shape (84468, 423535) and data type float64"
     ]
    }
   ],
   "source": [
    "x_train = tokenizer.texts_to_matrix(text_train, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(target_train, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel\n",
    "# import torch\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# # Tokenizer & Model\n",
    "# from transformers import GPT2TokenizerFast, pipeline\n",
    "\n",
    "# MODEL_NAME='aubmindlab/aragpt2-base'\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a90448524f656655376e589a7c5d3deb138177bb112388cb30ddfc6b5ff35d8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
